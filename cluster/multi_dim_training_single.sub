#!/bin/bash

# the very first line is a special directive for Bash shell, do not remove
# lines that start with "#SBATCH" are special directives for Slurm
# other lines that start with "#" are comments

#SBATCH --job-name=multi_dim_training                   ## job name
#SBATCH -p atlas                                        ## use free partition
#SBATCH --nodes=1                                       ## use 1 node, don't ask for multiple
#SBATCH --ntasks=1                                      ## ask for 1 CPU
#SBATCH --mem-per-cpu=1G                                ## ask for 1Gb memory per CPU
#SBATCH --error=cluster_output/%x.%A.err              ## Slurm error  file, %x - job name, %A job id
#SBATCH --out=cluster_output/%x.%A.out                ## Slurm output file, %x - job name, %A job id

# Run command hostname and assign output to a variable
hn=$(hostname)
echo "Running job on host $hn"
echo "-----------------------"

# load modules
ml purge
ml miniconda/3

# Activate conda env
conda init
conda activate piad

# Prepare reporting
mkdir -p saved_models_cluster
mkdir -p cluster_output

# Run python code
python3.12 multi_dim_training.py 128 0.0001 0.01 10 500000 0.1 0.0
